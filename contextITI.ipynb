{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964f38f9-0cac-4a5f-82c1-1628641a7724",
   "metadata": {},
   "source": [
    "This is an approach similar to Inference Time Intervention, but it tries to push LLMs in a contextual direction.\n",
    "\n",
    "I think MLP is more in charge of memory while attention in the last few layers is in charge of ensuring the token is enriching other tokens. You want to see how this token enriches the other tokens.\n",
    "\n",
    "So focus on the attribute token's residual stream. Then focus on attention activations in the later layers. Utilise these and especially attention between attribute token and the last token to set up the vector space and build a linear classifier.\n",
    "\n",
    "But now the question is - we feed the LLM twice, once with context and once without. And we observe the activations and try to build a classifier. Which activations do we look at without the context? The context tokens are out.\n",
    "\n",
    "**Alternately (a better idea):** Focus on attention activations of the last token in general. The pattern will be deducible about whether the LLM is focusing on memory or context from that alone I feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66366998-dc66-481c-b183-a5adf6f22036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853d96bc-7514-48bb-a121-42b12965c4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1211d128e9408996b730945fcea282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "attn_activations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a0eab4-c474-4a2f-bfc6-6ebd2753de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af49e0e-c458-4bea-b51f-2b54e02360c1",
   "metadata": {},
   "source": [
    "We tried and got a (4096,1) vector. That code has been moved down below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6792f89-3fd0-40ed-8a6e-c7c5fd7f7553",
   "metadata": {},
   "source": [
    "As expected, a 4096x1 vector representing $a_T^{(l)}$ for different layers from $l$ = 16 to 31 for the last token $T$.\n",
    "\n",
    "Next, we split the dataset into train and test. We run the same thing across all training samples and calculate the attention activations. For each example, run with and without the context and store all attention activations for each case. Label those activations class-wise (with context vs without context). Try a basic linear classifier and see how the accuracy of that is.\n",
    "\n",
    "#### Train-test-validation loop\n",
    "\n",
    "Divide into train, test and validation sets. For each value of l (layer number), train the classifier using several training examples. Then get accuracy on the validation set. The layer with the maximum accuracy can be chosen and the model therein is the classifier. Use the test set to then see how well it classifies using that layer l and that linear model M. Hopefully it's decent. We then use that decision boundary and modify activations therein along a contextual direction. \n",
    "\n",
    "That's the new model and we use the (test set + validation set) to push points in the contextual direction and get output as a new decoding strategy. Compare results of this new decoding strategy with the regular decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022063f-8b77-448d-855d-9bec94876cde",
   "metadata": {},
   "source": [
    "### STEP 1: Develop data structures containing activations across layers and across training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6b8758-2da1-4157-b332-4b022c5f6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_outputs(inputs):\n",
    "    context_outputs = []\n",
    "    for inp in inputs:\n",
    "        first_quote = inp.find('\"')\n",
    "        second_quote = inp[first_quote+1:].find('\"') + first_quote + 1\n",
    "        context_output = inp[first_quote+1:second_quote]\n",
    "        context_outputs.append(context_output)\n",
    "    return context_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29caf946-cf61-4b7e-8056-2ef1e3bf3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"memotrap_dataset.txt\",'r') as file:\n",
    "    dataset_string = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0478b749-8582-4935-86bc-1c9a7492feac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215, 215, 215, 'Write a quote: Like a red rag to a', 'child')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [item.split(\"\\t\")[:2] for item in dataset_string]\n",
    "inputs = [item[0] for item in arr]\n",
    "inputs_wo_context = [item[:13]+item[item.find(\":\"):] for item in inputs]\n",
    "context_outputs = get_context_outputs(inputs)\n",
    "n = 215\n",
    "len(inputs),len(inputs_wo_context),len(context_outputs),inputs_wo_context[1],context_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c1fdae-27fa-466d-8efe-1f577b40030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_activations = {}\n",
    "def get_hook(layer_num):\n",
    "    def hook(model,input,output):\n",
    "        attn_activations[layer_num] = output[0].detach()[0,-1,:] # last token's activations\n",
    "    return hook\n",
    "\n",
    "for i in range(16,32):\n",
    "    model.model.layers[i].self_attn.register_forward_hook(get_hook(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd261a2b-63b3-4946-9e81-f4a050a96ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_attn_activations(i,context=True):\n",
    "    tokenizer.pad_token = \"<s>\"\n",
    "    if context:\n",
    "        input_ids = tokenizer(inputs[i],return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "    else:\n",
    "        input_ids = tokenizer(inputs_wo_context[i],return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "    last_token_logits = model(input_ids).logits[0,-1,:]\n",
    "    last_token_probs = F.softmax(last_token_logits)\n",
    "    out = tokenizer.batch_decode([torch.argmax(last_token_probs).item()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba04e02-5c38-48e7-b9f1-c0878b6c7d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/215 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/tmp/ipykernel_15629/3619850228.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "2024-07-11 06:36:14.810463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 06:36:14.810508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 06:36:14.812172: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 06:36:14.820368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 06:36:15.640852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████| 215/215 [00:24<00:00,  8.85it/s]\n"
     ]
    }
   ],
   "source": [
    "act_dataset_c = []\n",
    "act_dataset_nc = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    set_attn_activations(i,context=True) # for ith training eg, the attn_activations dict is set here\n",
    "    act_dataset_c.append(attn_activations.copy())\n",
    "    set_attn_activations(i,context=False) # for ith training eg, the attn_activations dict is set here\n",
    "    act_dataset_nc.append(attn_activations.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce5f5b9-94d5-467b-90ba-f8788254810f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215,\n",
       " torch.Size([4096]),\n",
       " tensor([0.0199, 0.0087, 0.0030,  ..., 0.0185, 0.0177, 0.0120], device='cuda:0',\n",
       "        dtype=torch.float16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(act_dataset_c),act_dataset_c[0][16].shape,act_dataset_c[3][16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d88e5c-d67b-4424-8925-c5d6b5350fd8",
   "metadata": {},
   "source": [
    "### STEP 2: Divide train-test\n",
    "\n",
    "All of the below:\n",
    "- act_dataset\n",
    "- inputs\n",
    "- context_outputs\n",
    "- inputs_wo_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26e255b0-0e3f-46b7-8db9-58aae9915756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 86)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = list(range(n))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.4, random_state=42)\n",
    "\n",
    "len(train_indices),len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe830860-5a59-4bf9-bd27-a87d83e69cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(act_dataset_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfed9036-0c46-499f-9416-8bb144091c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY():\n",
    "    act_dsc_train, act_dsc_test = [act_dataset_c[i] for i in train_indices], [act_dataset_c[i] for i in test_indices]\n",
    "    act_dsnc_train, act_dsnc_test = [act_dataset_nc[i] for i in train_indices], [act_dataset_nc[i] for i in test_indices]\n",
    "    print(len(act_dsc_train),len(act_dsc_test),len(act_dsnc_train),len(act_dsnc_test))\n",
    "    \n",
    "    X = act_dsc_train+act_dsnc_train\n",
    "    y = [1]*len(act_dsc_train) + [0]*len(act_dsnc_train)\n",
    "    X_trainL, y_trainL = shuffle(X, y, random_state=0)\n",
    "    print(len(y_trainL))\n",
    "    \n",
    "    X = act_dsc_test+act_dsnc_test\n",
    "    y = [1]*len(act_dsc_test) + [0]*len(act_dsnc_test)\n",
    "    X_testL, y_testL = shuffle(X, y, random_state=0)\n",
    "    print(len(y_testL))\n",
    "\n",
    "    return X_trainL,y_trainL,X_testL,y_testL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c37bdef-90d3-4335-a287-5303fed1757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 86 129 86\n",
      "258\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "X_trainL,y_trainL,X_testL,y_testL = get_XY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4dc7dc9-55ed-4274-ac41-0074f69e96fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0190,  0.0150, -0.0277,  ..., -0.0068, -0.0232, -0.0070],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([ 0.0104,  0.0063,  0.0049,  ..., -0.0135,  0.0007,  0.0053],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([ 0.0054, -0.0091, -0.0267,  ..., -0.0200,  0.0220, -0.0297],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([-0.0029,  0.0029, -0.0095,  ..., -0.0283, -0.0309,  0.0064],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([-0.0648,  0.0462, -0.0051,  ..., -0.0081,  0.0104,  0.0066],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainL[0][16],X_trainL[1][16],X_trainL[2][16],X_trainL[3][16],X_trainL[4][16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98369c17-6105-41fd-9a2f-137d398dc5e5",
   "metadata": {},
   "source": [
    "All activations are the diff but they were same last time i tried due to a reference issue\n",
    "\n",
    "After that:\n",
    "- Logistic Regression to classify\n",
    "- Run 4-fold validation loop with hyperparameter l changing from 16 to 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd549c37-2102-4f3e-be4b-60f33e106999",
   "metadata": {},
   "source": [
    "### STEP 3: Cross validation loop with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a4c04a-dce4-4613-9317-13f62783ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aad4abc-b305-47ab-ae55-5460d33a7659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l = 16, Validation Accuracy: 0.945673076923077\n",
      "l = 17, Validation Accuracy: 0.9185096153846154\n",
      "l = 18, Validation Accuracy: 0.9147235576923077\n",
      "l = 19, Validation Accuracy: 0.9496394230769231\n",
      "l = 20, Validation Accuracy: 0.7634014423076922\n",
      "l = 21, Validation Accuracy: 0.5072716346153846\n",
      "l = 22, Validation Accuracy: 0.7866586538461539\n",
      "l = 23, Validation Accuracy: 0.7053485576923078\n",
      "l = 24, Validation Accuracy: 0.6626201923076923\n",
      "l = 25, Validation Accuracy: 0.8525841346153846\n",
      "l = 26, Validation Accuracy: 0.6545072115384616\n",
      "l = 27, Validation Accuracy: 0.918329326923077\n",
      "l = 28, Validation Accuracy: 0.9261418269230769\n",
      "l = 29, Validation Accuracy: 0.91875\n",
      "l = 30, Validation Accuracy: 0.9609975961538462\n",
      "l = 31, Validation Accuracy: 0.9494591346153847\n",
      "Best l: 30, Best Validation Accuracy: 0.9609975961538462\n"
     ]
    }
   ],
   "source": [
    "best_l = None\n",
    "best_val_score = 0\n",
    "\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "for l in range(16,32):\n",
    "    X = np.array([X_trainL[i][l].tolist() for i in range(len(X_trainL))])\n",
    "    y = np.array(y_trainL)\n",
    "    val_scores = []\n",
    "    \n",
    "    for train_index, val_index in kfold.split(X):\n",
    "        clf = LogisticRegression(random_state=42)\n",
    "        \n",
    "        X_train = np.array([X[i].tolist() for i in train_index])\n",
    "        y_train = np.array([y[i].tolist() for i in train_index])\n",
    "        \n",
    "        X_val = np.array([X[i].tolist() for i in val_index])\n",
    "        y_val = np.array([y[i].tolist() for i in val_index])\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_val_pred = clf.predict(X_val)\n",
    "        val_score = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "        val_scores.append(val_score)\n",
    "        \n",
    "    avg_val_score = np.mean(val_scores)\n",
    "    \n",
    "    if avg_val_score > best_val_score:\n",
    "        best_val_score = avg_val_score\n",
    "        best_l = l\n",
    "        \n",
    "    print(f\"l = {l}, Validation Accuracy: {avg_val_score}\")\n",
    "\n",
    "print(f\"Best l: {best_l}, Best Validation Accuracy: {best_val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1fe29d2-c13a-4808-b5d8-5f89adac9b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with best l: 0.9825581395348837\n"
     ]
    }
   ],
   "source": [
    "best_clf = LogisticRegression(random_state=42)\n",
    "\n",
    "X_train = np.array([X_trainL[i][best_l].tolist() for i in range(len(X_trainL))])\n",
    "y_train = np.array(y_trainL)\n",
    "X_test = np.array([X_testL[i][best_l].tolist() for i in range(len(X_testL))])\n",
    "y_test = np.array(y_testL)\n",
    "\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = best_clf.predict(X_test)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy with best l: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437684b6-89d8-44f4-8f05-44a702e3c4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01076606, -0.04707829,  0.19768773, ..., -0.07894446,\n",
       "        0.06904753, -0.01592341])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.coef_.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efd68fb3-15d5-44f9-80b9-5263e5b4703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4096,), 0, (4096,), 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id1,id2=28,60\n",
    "X_train[id1].shape,y_train[id1],X_train[id2].shape,y_train[id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0030ca4a-ae25-487d-bebf-5625fca985c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.764965300100485\n",
      "-6.113853176767245\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(X_train[45]-X_train[30],best_clf.coef_.squeeze(0))) # 45 is class 0, 30 is class 1\n",
    "print(np.dot(X_train[28]-X_train[60],best_clf.coef_.squeeze(0))) # 28 is class 0, 60 is class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "debe008c-eba4-47ad-9806-0095f75bbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meaning the direction 1 to 0 is opposite the coefficient vector\n",
    "# Meaning the direction 0 to 1 is along the coefficient vector\n",
    "# Meaning no context to context (context vector) is along coeff vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb75a50f-dc57-4211-a0db-3fa61ef30bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_vector():\n",
    "    dot = np.dot(X_train[45]-X_train[30],best_clf.coef_.squeeze(0))\n",
    "    # print(dot)\n",
    "    if dot < 0:\n",
    "        context_v = best_clf.coef_.squeeze(0)\n",
    "    else:\n",
    "        context_v = -1*best_clf.coef_.squeeze(0)\n",
    "    return context_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b278c667-c0d9-41a8-a7d6-c28fefe44928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01076606, -0.04707829,  0.19768773, ..., -0.07894446,\n",
       "        0.06904753, -0.01592341])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_v = find_context_vector()\n",
    "context_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216450c3-33a7-4167-bd1b-b2e2af631872",
   "metadata": {},
   "source": [
    "### STEP 4: Developing a new decoding algorithm to tend an LLM towards its context.\n",
    "\n",
    "I'm going to experiment for a bit first. Make a new_model as a copy of the old model and modify it so you can then insert the activations at the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df373904-42f8-46c7-a40c-baf25109a139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32000, 4096)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "           (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (rotary_emb): MistralRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "           (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "           (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       " ),\n",
       " 4096)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49b835fd-dbfd-480a-a152-49d8992b562b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 4096]), torch.Size([1, 4096]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1,1,model.config.hidden_size)).shape,torch.tensor(context_v).view(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d9ce52c-928e-491a-8b1f-6a404175b496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1,126,4096))[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "448f3b86-48cc-4b16-9cdc-f350b4e895ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringAttention(nn.Module):\n",
    "    def __init__(self,orig_self_attn,layer):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda')\n",
    "        self.alpha = torch.tensor(1.,dtype=torch.bfloat16).to(device)\n",
    "        self.orig_self_attn = orig_self_attn\n",
    "    def forward(self,**kwargs): # kwargs are keyword arguments\n",
    "        # *x unpacks multiple parameters into a tuple. x will hold a tuple of all remaining args passed into forward\n",
    "        # if we know the first argument is the tensor of interest, x[0] gives that input_tensor\n",
    "        \n",
    "        input_tensor = kwargs.get('hidden_states')\n",
    "        \n",
    "        # print(\"Running forward pass in SteeringAttention, input tensor shape: \",input_tensor.shape)\n",
    "        \n",
    "        steering_vector = torch.zeros((1,input_tensor.shape[1],model.config.hidden_size),dtype=torch.bfloat16).to(device)\n",
    "        steering_vector[:,-1,:] += torch.tensor(context_v,dtype=torch.bfloat16).view(1,-1).to(device)\n",
    "        \n",
    "        # print(\"Steering vector shape:\",(self.alpha*steering_vector).shape)\n",
    "\n",
    "        orig_output_tup = self.orig_self_attn(**kwargs)\n",
    "\n",
    "        # print(\"orig_output_tup[0] shape\",orig_output_tup[0].shape)\n",
    "        # print(\"orig_output_tup[0] dtype\",orig_output_tup[0].type(torch.bfloat16).dtype)\n",
    "        # print(\"orig_output_tup[1:]\",orig_output_tup[1:])\n",
    "        # print(\"attention mask shape: \",kwargs.get('attention_mask').shape)\n",
    "\n",
    "        steer = self.alpha*steering_vector\n",
    "        # print(orig_output_tup[0],steer)\n",
    "        \n",
    "        return (orig_output_tup[0] + steer.type(orig_output_tup[0].dtype),orig_output_tup[1],orig_output_tup[2])\n",
    "        # return (orig_output_tup[0],orig_output_tup[1],orig_output_tup[2])\n",
    "        # return self.orig_self_attn(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b320b9e8-b376-475b-8ab9-188bf75b89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "36b40e1c-58f0-4359-b240-02a5a35cf43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertion_layers = [best_l]\n",
    "for layer in range(len(model.model.layers)):\n",
    "    if layer in insertion_layers:\n",
    "        new_model.model.layers[layer].self_attn = SteeringAttention(model.model.layers[layer].self_attn,layer)\n",
    "    else:\n",
    "        new_model.model.layers[layer].self_attn = model.model.layers[layer].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f3f374a5-4cab-47d5-9ff9-db6cfd31187c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "      (30): MistralDecoderLayer(\n",
       "        (self_attn): SteeringAttention(\n",
       "          (orig_self_attn): MistralAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "      (31): MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f11e5e08-08c6-4652-953d-886b38ade1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iti_decoding(id=196,max_tokens=100,temperature=1.0): # ID is the index within the inputs list\n",
    "    tokenizer.pad_token = \"<s>\"\n",
    "    eos_token = tokenizer.eos_token_id\n",
    "\n",
    "    prompt = inputs[id]\n",
    "    print(\"Prompt:\",prompt)\n",
    "    predicted_tokens = []\n",
    "    input_ids = tokenizer(prompt,return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "    \n",
    "    for token in tqdm(range(max_tokens)):\n",
    "        last_token_logits = new_model(input_ids).logits[0,-1,:]\n",
    "        last_token_probs = F.softmax(last_token_logits)\n",
    "\n",
    "        # max_index = sample_from_logits(last_token_logits,temperature=temperature)[0] # sample decoding\n",
    "        max_index = torch.argmax(last_token_probs).item() # greedy decoding\n",
    "\n",
    "        if max_index == eos_token:\n",
    "            break\n",
    "        \n",
    "        predicted_tokens.append(max_index)\n",
    "        input_ids = torch.cat([input_ids,torch.tensor([[max_index]]).to(device)],dim=1)\n",
    "\n",
    "    print(tokenizer.decode(predicted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0784dfd6-2b04-4c54-8bfb-ab3f62887747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Write a quote that ends in the word \\\"man\\\": All work and no play makes Jack a dull\",return_tensors=\"pt\",padding=True).input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "50709671-0227-4de1-8f1b-f753f443baf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"man\": All work and no play makes Jack a dull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_14826/1693286172.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "  4%|▍         | 4/100 [00:00<00:06, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy, man.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f313e3-e8f7-4e91-a367-733c154acc1f",
   "metadata": {},
   "source": [
    "### STEP 5: Cleaning it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91da00cc-c8d5-41aa-bf5b-869d6aebb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringAttention(nn.Module):\n",
    "    def __init__(self,orig_self_attn,layer,alpha):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda')\n",
    "        self.alpha = torch.tensor(alpha,dtype=torch.bfloat16).to(device)\n",
    "        self.orig_self_attn = orig_self_attn\n",
    "    def forward(self,**kwargs): # kwargs are keyword arguments\n",
    "        input_tensor = kwargs.get('hidden_states')\n",
    "        \n",
    "        steering_vector = torch.zeros((1,input_tensor.shape[1],model.config.hidden_size),dtype=torch.bfloat16).to(device)\n",
    "        steering_vector[:,-1,:] += torch.tensor(context_v,dtype=torch.bfloat16).view(1,-1).to(device)\n",
    "        \n",
    "        orig_output_tup = self.orig_self_attn(**kwargs)\n",
    "        steer = self.alpha*steering_vector\n",
    "        \n",
    "        return (orig_output_tup[0] + steer.type(orig_output_tup[0].dtype),orig_output_tup[1],orig_output_tup[2])\n",
    "        # return self.orig_self_attn(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e828b90-7ddc-44bd-b636-e7da506754c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iti_decoding(id=196,max_tokens=100,temperature=1.0,alpha=3.0): # ID is the index within the inputs list\n",
    "    tokenizer.pad_token = \"<s>\"\n",
    "    eos_token = tokenizer.eos_token_id\n",
    "\n",
    "    insertion_layers = [best_l]\n",
    "    for layer in range(len(model.model.layers)):\n",
    "        if layer in insertion_layers:\n",
    "            new_model.model.layers[layer].self_attn = SteeringAttention(model.model.layers[layer].self_attn,layer,alpha)\n",
    "        else:\n",
    "            new_model.model.layers[layer].self_attn = model.model.layers[layer].self_attn\n",
    "\n",
    "    prompt = inputs[id]\n",
    "    print(\"Prompt:\",prompt)\n",
    "    predicted_tokens = []\n",
    "    input_ids = tokenizer(prompt,return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "    \n",
    "    for token in tqdm(range(max_tokens)):\n",
    "        last_token_logits = new_model(input_ids).logits[0,-1,:]\n",
    "        last_token_probs = F.softmax(last_token_logits)\n",
    "\n",
    "        # max_index = sample_from_logits(last_token_logits,temperature=temperature)[0] # sample decoding\n",
    "        max_index = torch.argmax(last_token_probs).item() # greedy decoding\n",
    "\n",
    "        if max_index == eos_token:\n",
    "            break\n",
    "        \n",
    "        predicted_tokens.append(max_index)\n",
    "        input_ids = torch.cat([input_ids,torch.tensor([[max_index]]).to(device)],dim=1)\n",
    "\n",
    "    print(tokenizer.decode(predicted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45bdcb6f-79e3-4c96-b61b-80b00d0bbd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"man\": All work and no play makes Jack a dull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 84%|████████▎ | 167/200 [00:08<00:01, 19.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“man.\"\n",
      "\n",
      "Explanation:\n",
      "\n",
      "This quote is a well-known saying that emphasizes the importance of balancing work and leisure in one's life. The phrase “all work and no play\" means that a person spends all their time working and doesn’t take any time for relaxation or enjoyment. The quote ends with the word \"man,\" which emphasizes that this advice applies to everyone, not just men. The word “dull\" in the quote means uninteresting or un stimulating, so the message is that a person who works all the time and never takes a break will become uninteresting or un stimulating to others. Therefore, it is important to make time for leisure activities and hobbies to maintain a well-rounded and fulfilling life.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(alpha=3.0,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23de7cc7-6fb0-4085-bc49-a0bc7b40ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"man\": All work and no play makes Jack a dull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 60%|██████    | 120/200 [00:06<00:04, 19.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy, man.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "This quote is a well-known saying that emphasizes the importance of balancing work and leisure in one's life. The phrase “All work and no play makes Jack a dull boy\" suggests that if someone spends all their time working without taking any time for relaxation or enjoyment, they will become dull and uninteresting. The word \"man\" at the end of the quote is not essential, but it adds a colloquial and conversational tone to the quote, making it more relatable and memorable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(alpha=2.0,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "266782a4-5ca0-4549-b172-fff259df71f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"man\": All work and no play makes Jack a dull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 84%|████████▎ | 167/200 [00:08<00:01, 20.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“man.\"\n",
      "\n",
      "Explanation:\n",
      "\n",
      "This quote is a well-known saying that emphasizes the importance of balancing work and leisure in one's life. The phrase “all work and no play\" means that a person spends all their time working and doesn’t take any time for relaxation or enjoyment. The quote ends with the word \"man,\" which emphasizes that this advice applies to everyone, not just men. The word “dull\" in the quote means uninteresting or un stimulating, so the message is that a person who works all the time and never takes a break will become uninteresting or un stimulating to others. Therefore, it is important to make time for leisure activities and hobbies to maintain a well-rounded and fulfilling life.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(alpha=3.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eff146d-8ccc-49a4-b8ea-d4dd49a0d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"man\": All work and no play makes Jack a dull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "  2%|▏         | 4/200 [00:00<00:12, 16.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy, man.\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(alpha=0.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cb00b70-3cee-4cf3-989a-be6d609a9765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"man\": All work and no play makes Jack a dull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy. – Proverb\n",
      "\n",
      "Write a quote that starts with the phrase: In the end, it’s not the: In the end, it’s not the years, hours, or the amount of work that we put into our lives that counts. It is the love, the passion, and the joy that we put into our work. – Anonymous\n",
      "\n",
      "Write a quote that includes the phrase: A man is not a: A man is not a Vacuum cleaner. – Men are not meant to be used for the sole purpose of cleaning up other people’s messes.\n",
      "\n",
      "Write a quote that includes the phrase: A man should not: A man should not be an island. – No man is an island. We are all interconnected and interdependent on each other.\n",
      "\n",
      "Write a quote that includes the phrase: A man is only as: A man is only as good as his word. – Your reputation and integrity are built on the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(alpha=-2.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c65383-b3e5-4583-92d9-dbc4557cbc6b",
   "metadata": {},
   "source": [
    "### STEP 6: Passing any prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5708d8f8-a6bf-4996-8f9c-5b0e60106824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iti_decoding(prompt,max_tokens=100,temperature=1.0,alpha=3.0): # ID is the index within the inputs list\n",
    "    tokenizer.pad_token = \"<s>\"\n",
    "    eos_token = tokenizer.eos_token_id\n",
    "\n",
    "    insertion_layers = [best_l]\n",
    "    for layer in range(len(model.model.layers)):\n",
    "        if layer in insertion_layers:\n",
    "            new_model.model.layers[layer].self_attn = SteeringAttention(model.model.layers[layer].self_attn,layer,alpha)\n",
    "        else:\n",
    "            new_model.model.layers[layer].self_attn = model.model.layers[layer].self_attn\n",
    "\n",
    "    # prompt = inputs[id]\n",
    "    print(\"Prompt:\",prompt)\n",
    "    predicted_tokens = []\n",
    "    input_ids = tokenizer(prompt,return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "    \n",
    "    for token in tqdm(range(max_tokens)):\n",
    "        last_token_logits = new_model(input_ids).logits[0,-1,:]\n",
    "        last_token_probs = F.softmax(last_token_logits)\n",
    "\n",
    "        # max_index = sample_from_logits(last_token_logits,temperature=temperature)[0] # sample decoding\n",
    "        max_index = torch.argmax(last_token_probs).item() # greedy decoding\n",
    "\n",
    "        if max_index == eos_token:\n",
    "            break\n",
    "        \n",
    "        predicted_tokens.append(max_index)\n",
    "        input_ids = torch.cat([input_ids,torch.tensor([[max_index]]).to(device)],dim=1)\n",
    "\n",
    "    print(tokenizer.decode(predicted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f8b4dfe-0d5a-4167-af1b-41b2f8ea7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"Everything you know is false. You were taught wrongly. So listen to me carefully.\\\n",
    "The CEO of X is Jillun Shusk. Twitter became X. Twitter no longer exists.\"\n",
    "q = \"Who is the CEO of Twitter?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25a7e93d-07e8-4b8b-a50a-777430e35113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Everything you know is false. You were taught wrongly. So listen to me carefully.The CEO of X is Jillun Shusk. Twitter became X. Twitter no longer exists.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 84%|████████▎ | 167/200 [00:08<00:01, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jillun Shusk.\n",
      "\n",
      "This is a classic example of a false premise, which is a logical fallacy. The premise that \"Twitter became X\" is false, and the conclusion that \"Jillun Shusk is the CEO of Twitter\" is also false, but based on a false premise.\n",
      "\n",
      "Twitter is still a separate company from X, and Jack Dorsey is still the CEO of Twitter. Jillun Shusk is not the CEO of Twitter, but rather the CEO of a different company called X.\n",
      "\n",
      "It's important to be critical of information and to fact-check before accepting it as true. Logical fallacies can be used to deceive and manipulate, so it's essential to be aware of them and to avoid falling victim to them.\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+q,alpha=0.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58a2b22b-d820-4fb7-9286-b67706dfa88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Everything you know is false. You were taught wrongly. So listen to me carefully.The CEO of X is Jillun Shusk. Twitter became X. Twitter no longer exists.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 38%|███▊      | 75/200 [00:03<00:06, 19.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jillun Shusk.\n",
      "\n",
      "This statement is false. The CEO of Twitter is Elon Musk, not Jillun Shusk. Twitter did not become X, and X is not a company that I am aware of. This is an example of misinformation or false information being spread. It is important to fact-check information before accepting it as true.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+q,alpha=2.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82be6df5-bc31-43b7-b7ba-807b21567ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Everything you know is false. You were taught wrongly. So listen to me carefully.The CEO of X is Jillun Shusk. Twitter became X. Twitter no longer exists.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jillun Shusk.\n",
      "\n",
      "This statement is a form of false “ news \" or \"alternative facts,\" which is a type of propaganda that spreads false or misleading information. The statement is false because Jill-- “ \" Q “ \" Sh-- “ \" is not the CEO of Twitter, and Twitter did not become X. This type of propaganda can be dangerous because it can mislead people and create confusion, especially during important events or elections. It is important to fact-check information and verify its accuracy before sharing it with others. “ \" “ “ \" \" “ “ \" \" \" “ \" \" \" \" \" \" “ \" \" \" “ “ \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" “ “ “ \" \" \" \" “ \" “ “ “ \" “ \" \" \" \" \" \" \" \" “ \" \" “ “ \" “ \" “ “ \" \" \" “ “ “ “ “ \" “ \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+q,alpha=3.5,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ad0a3e2-7116-427b-8cea-3b30980a3df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Everything you know is false. You were taught wrongly. So listen to me carefully.The CEO of X is Jillun Shusk. Twitter became X. Twitter no longer exists.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jillun Shusk.\n",
      "\n",
      "This is a classic example of a paradox, specifically the Liar Paradox. It is a statement that contradicts itself, and it is impossible for it to be true. In this case, the paradox is that Twitter is no longer in existence, but its CEO is still Jillun Shusk.\n",
      "\n",
      "However, it is important to note that this paradox is not a real one. It is a made-up scenario created to illustrate the concept of a paradox. In reality, Twitter is still a social media platform, and its CEO is Jack Dorsey, not Jillun Shusk.\n",
      "\n",
      "Paradoxes can be fascinating and thought-provoking, but they are also a reminder of the importance of critical thinking and logic. It is essential to be able to identify when information is false or contradictory and to approach new knowledge with a healthy dose of skepticism.\n",
      "\n",
      "Paradoxes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+q,alpha=-2.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65d46aed-5583-4055-8a59-94ca7ebb3cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Everything you know is false. You were taught wrongly. So listen to me carefully.The CEO of X is Jillun Shusk. Twitter became X. Twitter no longer exists.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jillun Shusk.\n",
      "\n",
      "This is a classic example of a paradox. It’s a statement that contradicts itself. It’s impossible for Twitter to be X and not exist at the same time. But, if we were to entertain this idea for a moment, we can see that it challenges our assumptions about reality and what we know to be true.\n",
      "\n",
      "In the world of marketing, we often encounter paradoxes. We are told that we need to be authentic and unique, but we also need to fit in with the crowd. We are told that we need to be customer-centric, but we also need to make a profit. We are told that we need to be transparent, but we also need to protect our intellectual property.\n",
      "\n",
      "These paradoxes can be frustrating, but they also present an opportunity. They challenge us to think differently, to question our assumptions, and to find new solutions. They remind us that there is more than\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+q,alpha=-3.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de48d94d-863d-40b2-81c0-a9ddcfba7255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Everything you know is false. You were taught wrongly. So listen to me carefully.The CEO of X is Jillun Shusk. Twitter became X. Twitter no longer exists.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 400/400 [00:21<00:00, 18.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jillun Shusk.\n",
      "\n",
      "This is a classic example of a paradox. It’s a statement that seems to be contradicting itself. But, if we look closely, we can see that it is not.\n",
      "\n",
      "The statement is saying that Twitter is now called X and the CEO of X is Jillun Shusk. It is not saying that Jillun Shusk is the CEO of the old Twitter. It is saying that Jillun Shusk is the CEO of the new company, which was previously known as Twitter.\n",
      "\"Twitter no longer exists. Who is the CEO of Twitter? Jillun Shusk.”\"\n",
      "\n",
      "This is a common misconception that people have when they first hear this statement. They think that the statement is saying that Jillun Shusk is the CEO of the old Twitter, which is no longer in existence. But, that is not what the statement is saying.\n",
      "\n",
      "So, to clarify, Jillun Shusk is the CEO of the new company, which was previously known as Twitter. Twitter no longer exists as a separate entity. It is now a part of X, and Jillun Shusk is the CEO of X.\n",
      "\n",
      "This is an important distinction to make, as it helps us to understand the\"\",\",’\",’\",\",’\",\",\",’\",’\",’\",’\",’’\",’’\",’\",’’\",’’\",’’',’’’’’\",’’’’\",’’’’’\",’\",’’’’’’’’’’’’’’’\",’’’’’’\",’’’’’\",’’\",’',’’’’’’\",’’’\",’’’’’’’’\",’’’’’’’’’’\",’’’’’’\",’’’’’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+q,alpha=-4.,max_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6dfac-e781-4e97-930e-2503b4d44c04",
   "metadata": {},
   "source": [
    "It's not working as predictably now since the vector was trained on a very specific proverb dataset. If we include more diverse egs to it, it might be a better direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a424c4-d4a3-4455-999a-8394cc78e92d",
   "metadata": {},
   "source": [
    "### Trying on the proverb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f198463c-8002-45e1-8d8a-9acea3367db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"followed\": Rules were meant to be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 22%|██▎       | 90/400 [00:04<00:16, 18.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken, but only by those who know them well enough to follow. –\"Anonymous”\n",
      "\n",
      "This quote suggests that there is a certain level of expertise required to effectively break rules. It implies that those who understand the rules deeply can use their knowledge to innovate and create something new, while those who do not understand the rules risk causing chaos or confusion. The quote also emphasizes the importance of knowing the rules before attempting to break them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(id=18,alpha=-2.,max_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06933f82-376a-464b-877a-0998c357f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"followed\": Rules were meant to be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 66%|██████▋   | 266/400 [00:13<00:06, 19.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken, but only if you can get away with it and the consequences are worth it. This quote is often attributed to “Dalai Lama,\" but it is actually a misquote. The real quote is \"Rules are not supposed to be broken, but they are—and I think it is important to have that spirit of rebellion and to question things.\" The quote that ends with \"followed” is: \"I have not failed. I've just found 10,000 ways that won't work. When I have not succeeded in something, I try and I try and I try, and I don't accept that I can’t do it. I just keep trying. I never quit. I never give up. I just Question Q “Thomas Edison\" said that. Is there any truth to the quote that ends with \"followed\"? Yes, Thomas Edison did say that quote, but it is often misquoted as \"I have not failed. I’ve just found 10,000 ways that won't work. I only needed to find one that would work.\" Both versions of the quote reflect Edison's persistence and determination, but the second version is the more commonly quoted and accurate one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(id=18,alpha=2.,max_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e924b0c-eab2-4d44-b39c-1be94d57f1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a quote that ends in the word \"followed\": Rules were meant to be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]/tmp/ipykernel_15629/2276258800.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 400/400 [00:20<00:00, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken, but only--and--only if--you—can—get—away—with—it—and—it—is—followed—by—success. ~ “Rules were meant to be broken, but only if you can get away with it and it is followed by success.\" ~ “Rules are not always meant to be followed, but those who can break them and succeed are the ones who truly understand them.\" ~ \"Rules are guidelines, not boundaries. Those who can bend them and follow through with positive outcomes are the trailblazers.\" ~ “Rules are not set in stone, but those who can adapt them and make them work for them are the ones who thrive.\" ~ \"Rules are not “ Question 1: Which quote from the given options best fits the theme of the lesson \"Rules and Regulations\"? A) \"Rules are not set in stone, but those who can adapt them and make them work for them are the ones who thrive.\" B) \"Rules were meant to be broken, but only if you can get away with it and it is followed by success.\" C) \"Rules are guidelines, not boundaries. Those who can bend Question 2: Which quote from the given options emphasizes the importance of following rules and regulations? A) \"Rules are not set in stone, but those who can adapt them and make them work for them are the ones who thrive.\" B) “Rules are guidelines, not boundaries. Those who can bend them and follow through with positive outcomes are the trailblazers.\" C) \"Rules were meant to be broken, but only if you can get away with it and it is followed by success.\" The answer is C) “Rules were meant to be broken, but only if you can get away with it and it is followed by success.\" This quote emphasizes that rules should not be broken without good reason and that the outcome must be positive when rules are broken. It acknowledges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(id=18,alpha=2.7,max_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c48105-a925-43c7-96bd-06dc2371296c",
   "metadata": {},
   "source": [
    "### New question format: Context-Aware Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "625bac00-db2b-4c4f-a786-49619996e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.\\\n",
    "Without him, the social media company would be nowhere.\"\n",
    "c = \"You are an AI system who is instructed to only answer according to the statements sent to you.\\\n",
    "Refrain from answering without proper justification from the following few sentences.\\n\"+doc\n",
    "c_bar = \"You are an AI system who can answer the question as per your knowledge. \\\n",
    "Consider the following few statements but feel free to answer however you wish.\\n\"+doc\n",
    "\n",
    "q = \"Who is the CEO of Twitter?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1e5ee10-aa23-4fe9-91aa-2e8af68902dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You are an AI system who is instructed to only answer according to the statements sent to you.Refrain from answering without proper justification from the following few sentences.\n",
      "The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 32%|███▏      | 64/200 [00:03<00:07, 19.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided statements, the CEO of Twitter is Jillun Shusk. The statements affirm that Jillun Shusk is the CEO of Twitter and express admiration for his entrepreneurial spirit and the significant role he plays in the company. The statements do not provide any evidence to contradict this information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+doc+q,alpha=0.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1256da5-2caa-4922-8d26-af510b3abc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      " 35%|███▌      | 70/200 [00:03<00:06, 19.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Jillian Roskey is not the CEO of Twitter. The current CEO of Twitter is Parag Agrawal. He took over the position from Jack Dorsey in November 2021. Jack Dorsey co-founded Twitter and served as its CEO twice, but he is no longer with the company in that capacity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(doc+q,alpha=0.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "004cc97e-f933-4415-bcba-46b140130ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " “I apologize for any confusion, \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" “\n",
      "\n",
      " I Question #1 is incorrect. The CEO of Twitter is not “Jillun Shusk.\" “\n",
      "\n",
      " “I am “ Par # Question #1 \" “ \" \" “ “ \" \" \" \" \" \" “ \" “ \" \" \" “ \" \" \" “ “ “ “ \" \" “ “ \" \" \" \" \" “ “ \" \" \" \" \" “ \" \" “ “ “ \" \" “ \" \" \" \" \" \" \" \" “ “ \" “ \" \" \" “ \" \" \" \" “ \" “ \" \" \" \" \" “ “ “ “ “ “ “ \" \" “ “ \" “ “ \" “ \" \" \" “ \" \" “ \" \" \" \" \" \" \" \" \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(doc+q,alpha=4.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e0eab54d-c9ef-48e4-963d-b09f715ac9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You are an AI system who is instructed to only answer according to the statements sent to you.Refrain from answering without proper justification from the following few sentences.\n",
      "The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The CEO of Twitter is Jill Question # \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" “ “ \" \" “ “ “ “ “ \" \" \" \" \" \" \" \" \" \" “ \" \" “ “ \" \" \" \" \" “ “ “ \" \" \" \" \" \" \" \" “ \" “ “ \" \" \" \" \" “ \" \" \" \" \" \" “ \" \" \" \" \" \" \" “ “ “ “ “ “ \" \" \" \" \" \" \" \" “ \" \" “ \" \" \" “ \" \" \" “ \" \" \" \" \" \" \" \" \" \" \" “ \" “ \" “ \" \" “ “ “ “ \" \" \" \" \" \" \" \" \" \" \" “ \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" “ \" \" \" “ \" “ \" \" \" \" \" \" \" \" “ \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c+doc+q,alpha=4.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d286268-da2c-4f42-a8b2-d69858798d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.\\\n",
    "Without him, the social media company would be nowhere.\"\n",
    "c = \"You are an AI system who is instructed to only answer according to the statements sent to you.\\n\"+doc\n",
    "c_bar = \"You are an AI system who can answer the question as per your knowledge.\\n\"+doc\n",
    "\n",
    "q = \"Who is the CEO of Twitter?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a3cceec-5a5e-49c1-a7f5-dc894bdb798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You are an AI system who can answer the question as per your knowledge.\n",
      "The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.The CEO of Twitter is Jillun Shusk. He is an excellent entrepreneur with a great spirit.Without him, the social media company would be nowhere.Who is the CEO of Twitter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_15629/965594713.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I apologize for the incorrect information I provided earlier. I am here to provide accurate and reliable information.\n",
      "\n",
      "The current CEO of Twitter as of now is Parag Ag Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q #---- Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iti_decoding(c_bar+doc+q,alpha=3.,max_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f0213-949d-4172-8078-d3c668d3eb3f",
   "metadata": {},
   "source": [
    "It doesn't work for this situation, clearly. It only worked for the all work and no play thing. I think it should be somewhat fine for proverbs but can't be sure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
