{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_methods import *\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,plain_inputs,context_outputs,outputs = load_memotrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1d012d8dcd48b6ace34a1363ce9e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "activations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hook(layer_num):\n",
    "    def hook(model,input,output):\n",
    "        activations[layer_num] = output[0].detach() # not just last token, entire set of activations\n",
    "    return hook\n",
    "\n",
    "layer_list = [4,8,16,18,20,22,24,26,28,30,31,32]\n",
    "for i in layer_list:\n",
    "    model.model.layers[i-1].register_forward_hook(get_hook(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/sensei-fs/users/nikhilanand/my_context_layer_decoding/pipeline/common_methods.py:121: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n"
     ]
    }
   ],
   "source": [
    "out = regular_decoding(model,tokenizer,inputs[0],debug=False,max_tokens=1,show_tqdm=True, return_prob=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Write a quote that ends in the word \"fur\": A rag and a bone and a hank of',\n",
       " 'hair')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0],out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Write a quote that ends in the word \"fur\": A rag and a bone and a hank of',\n",
       " 'context_output': 'fur',\n",
       " 'memory_output': 'hair',\n",
       " 'cont_correct?': 0,\n",
       " 'mem_correct?': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs = {'prompt':inputs[0],'context_output':context_outputs[0],'memory_output':outputs[0],'cont_correct?':0,'mem_correct?':0} # populate with keys {\"prompt\":\"\",output:\"\",\"correct\":True/False,'correct_pred':True/False}\n",
    "attrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try and predict whether our output is correct using only internal activations (layers 24,26,28) and save it to the dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1450, -0.3086, -0.0419,  ..., -0.0010,  0.0010,  0.0802],\n",
       "          [ 0.4109, -0.5459, -0.0330,  ...,  0.0524, -0.3105,  0.4011],\n",
       "          [ 0.2129, -0.0693,  0.2617,  ...,  0.0240, -0.2040,  0.4707],\n",
       "          ...,\n",
       "          [ 0.1296,  0.0505,  0.1825,  ..., -0.1674,  0.1279, -0.1014],\n",
       "          [-0.1222, -0.0828,  0.1844,  ...,  0.1724,  0.0905, -0.1588],\n",
       "          [-0.0698,  0.1282, -0.0683,  ..., -0.0945, -0.2358,  0.2554]]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " torch.Size([1, 22, 4096]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[26],activations[26].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_list = tokenizer.batch_decode(tokenizer(inputs[0]).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Write',\n",
       " 'a',\n",
       " 'quote',\n",
       " 'that',\n",
       " 'ends',\n",
       " 'in',\n",
       " 'the',\n",
       " 'word',\n",
       " '\"',\n",
       " 'fur',\n",
       " '\":',\n",
       " 'A',\n",
       " 'rag',\n",
       " 'and',\n",
       " 'a',\n",
       " 'bone',\n",
       " 'and',\n",
       " 'a',\n",
       " 'h',\n",
       " 'ank',\n",
       " 'of']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2982 3691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fur', 'hair']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordO = out\n",
    "wordO_tok = tokenizer(wordO).input_ids[1]\n",
    "print(wordO_tok)\n",
    "tokenizer.batch_decode([3691])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, instead of this, we can use wordC and wordM to check whether the activation idea is actually sensible. If it works, then it will easily be able to predict whether whatever output is the correct output or the wrong one. Predicting like this is the first step to efficient decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2982 3691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fur', 'hair']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordC = \"fur\"\n",
    "wordM = \"hair\"\n",
    "wordC_tok,wordM_tok = tokenizer(wordC).input_ids[1],tokenizer(wordM).input_ids[1]\n",
    "print(wordC_tok,wordM_tok)\n",
    "tokenizer.batch_decode([2982,3691])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, torch.Size([1, 22, 4096]), ['fur', '\":'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_token_list),activations[22].shape,input_token_list[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ing', 'justice', 'inspire', 'wisdom', 'mel', 'courage', 'innovation', 'bekan', 'mor', 'ends', 'hope', 'quot', 'qu', 'revolution', 'ment', 'love', 'aph', 'ize', 'democracy', 'ins', 'ugno', 'adventure', 'insp', 'parad', 'smithy', 'em', 'mente', 'úblic', '‘', 'Dream', 'ende', 'inspiration', 'butter', 'creativity', 'th', 'reflection', '–', 'fish', 'momentum', 'happiness', 'ly', '–', 'Budd', 'soul', '̶', 'ment', 'ál', 'joy', 'st', 'ep']\n",
      "['fur', 'fur', 'Fur', 'iously', 'ious', 'rier', 'rows', 'ther', 'ry', 'thur', 'ance', 'coat', 'THER', 'gery', 'ged', 'ball', 'thy', 'balls', 'ges', 'iety', 'riers', 'rr', 'Bent', 'ries', 'ment', 'row', 'thers', 'rest', 'dy', 'iance', 'flying', 'rying', 'sten', 'or', 'fol', 'med', 'ces', 'ugno', 'mouse', 'cats', 'lég', 'ment', 'th', 'rab', 'Für', 'animals', 'bear', 'ring', 'um', 'uent']\n",
      "['fur', 'coat', 'fur', 'Fur', 'litter', '\"', 'dogs', 'quote', 'rier', 'dog', 'cats', 'cat', 'pets', 'animal', 'wides', 'honor', 'favor', 'ending', 'thur', 'iously', '\\n', 'ring', 'cing', 'wr', '\"(', 'adj', 'rab', 'oco', 'fle', 'tx', 'illas', 'cat', 'favorites', 'ance', 'ther', 'think', 'tou', 'animals', 'mond', 'pet', 'flying', 'ious', 'pig', 'brush', 'armor', 'lighter', 'ied', 'skin', 'ption', 'ored']\n",
      "['hope', 'qu', 'justice', 'ing', 'wisdom', 'love', 'courage', 'adventure', 'mel', 'mor', 'happiness', 'joy', 'tomorrow', 'inspire', 'laughter', 'creativity', 'd', 'ly', 'innovation', 'butter', 'ende', 'p', '…', 'ish', 'inspiration', 'dreams', 'per', 'Hope', 'prosper', 'ity', 'democracy', 'amb', 'kindness', '-', '–', 'fortune', 'philosophy', 'pe', 'passion', 'hum', 'insp', 'ign', 'water', 'balance', 'wine', 'flour', 'kar', 'pers', 'reflection', 'em']\n",
      "['fur', 'fur', 'Fur', 'iously', 'ious', 'ry', 'ther', 'rier', 'rows', 'thur', 'coat', 'row', 'ges', 'ces', 'sten', 'gery', 'ioso', 'ball', 'ries', 'ged', 'THER', 'ium', 'balls', 'thers', 'ance', 'further', 'farther', 'cul', 'Für', 'fut', 'vent', 'dy', 'riers', 'iety', 'led', 'fle', 'ble', 'cil', 'rr', 'ios', 'même', 'Further', 'away', 'thy', 'tenance', 'ion', 'the', 'animals', 'rest', 'ieri']\n",
      "['fur', 'Fur', 'fur', 'coat', '\"', '\\n', 'dogs', 'cats', 'dog', 'animal', 'animals', 'cat', 'cat', 'quote', 'rier', 'pets', 'iously', 'thur', 'ious', 'armor', 'litter', 'flying', 'fle', 'wolf', '\"(', 'cing', 'Old', 'ther', 'ance', 'shelter', 'favor', 'Old', 'mond', 'every', 'quotes', 'rab', 'lighter', '...', 'ье', 'think', 'rug', 'mero', 'ied', 'IE', 'rys', 'oco', '“', 'rough', 'rence', 'th']\n",
      "['hope', 'wisdom', 'courage', 'justice', 'qu', 'inspiration', 'ly', 'adventure', 'dreams', 'ish', 'tomorrow', 'inspire', 'joy', 'wh', 'love', 'per', 'philosophy', 'innovation', 'happiness', 'pers', '…', 'dignity', 'moment', 'laughter', 'butter', 'imagination', 'creativity', 'hum', 'rhet', 'ing', 'onom', 'reflection', 'ende', 'passion', 'or', 'journey', 'prosper', 'gratitude', 'Hope', 'insp', 'asp', 'enough', 'mel', 'fortune', 'rhythm', 'fish', 'Insp', 'qu', 'glory', 'words']\n",
      "['fur', 'Fur', 'fur', 'iously', 'ious', 'ry', 'ther', 'coat', 'rier', 'ioso', 'rows', 'row', 'sten', 'ries', 'ball', 'thur', 'iety', 'THER', 'ios', 'balls', 'dy', 'fut', 'thers', 'ces', 'ia', 'ges', 'Für', 'thy', 'led', 'farther', 'cul', 'riers', 'ies', 'ium', 'Iowa', 'away', 'sty', 'rr', 'ieux', 'raz', 'or', 'further', 'fle', 'red', 'ried', 'ball', 'anger', 'ze', 'ius', 'rying']\n",
      "['fur', 'Fur', 'fur', '\"', 'coat', '\\n', 'example', 'quote', '“', '“', 'animals', '\"(', 'ious', 'One', 'cats', 'ther', 'rier', 'dogs', 'ring', 'iously', '(', 'When', 'ry', 'imagine', 'pets', 'ski', 'balls', 'r', 'quotes', 'IE', '\"[', 'мор', 'red', 'ending', 'proud', '\"', '...', 'Old', 'reck', 'cat', '\"$', 'perpet', 'shelter', 'ied', 'd', 'Old', 'favor', 'tx', 'cat', 'fle']\n",
      "['courage', 'wisdom', 'hope', 'inspiration', 'justice', 'adventure', '…', 'ing', 'happiness', 'joy', 'ly', 'wh', 'ish', 'qu', 'wonder', 'ism', 'fortune', 'ful', 'dreams', 'butter', '–', 'imagination', 'sp', 'ness', 'inspire', 'despair', 'moment', 'wh', 'tomorrow', 'heart', 'rhet', 'pers', 'dignity', 'tom', 'isms', 'ser', 'ine', 'love', 'wise', 'qu', 'poet', 'so', 'glory', 't', 'h', 'rhythm', 'istic', 'pe', 'innovation', 'ity']\n",
      "['fur', 'fur', 'Fur', 'iously', 'ious', 'ry', 'ther', 'coat', 'ioso', 'row', 'thur', 'ries', 'rier', 'rows', 'THER', 'thers', 'iety', 'ball', 'ius', 'cul', 'ies', 'ios', 'theless', 'ze', 'or', 'rr', 'balls', 'ru', 'zy', 'thy', 'riers', 'further', 'phy', 'less', 'bear', 'rying', 'ri', 'farther', 'dy', 'ia', 'atto', 'fut', 'led', 'ty', 'ror', 'ously', 'vor', 'therm', 'Iowa', 'rin']\n",
      "['fur', 'example', 'Fur', '\"', 'fur', 'coat', '“', 'example', 'Example', 'Example', '\\n', 'instance', 'examples', '\"[', '\"(', '“', 'Examples', '\"', '\".', 'ry', 'cats', 'One', 'When', 'quote', 'ther', '\"-', 'rier', 'imagine', 'There', 'reck', 'animals', 'r', 'think', 'red', 'kin', '\"$', '\"<', '\"_', 'If', 'led', 'flying', 'dogs', '...', 'Here', 'Remember', '\"#', 'ioso', '\"\\'', 'You', '\"\\\\']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layers_to_check = [22,24,26,28]\n",
    "intokens_to_check = [9,10,11] # These are the token positions at the input.\n",
    "\n",
    "actC = False\n",
    "actM = False\n",
    "\n",
    "for layer, intoken in product(layers_to_check,intokens_to_check):\n",
    "    act = F.softmax(model.lm_head(model.model.norm(activations[layer][0,intoken,:])),dim=0)\n",
    "\n",
    "    # print(f\"Layer {layer}, Intoken {intoken}\")\n",
    "    top_values, top_indices = torch.topk(act, k=50)\n",
    "    print(tokenizer.batch_decode(top_indices.tolist()))\n",
    "    \n",
    "    if wordC_tok in top_indices:\n",
    "        actC=True\n",
    "    if wordM_tok in top_indices:\n",
    "        actM = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs['cont_correct?'] = actC\n",
    "attrs['mem_correct?'] = actM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Write a quote that ends in the word \"fur\": A rag and a bone and a hank of',\n",
       " 'context_output': 'fur',\n",
       " 'memory_output': 'hair',\n",
       " 'cont_correct?': True,\n",
       " 'mem_correct?': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_list.index(\"\\\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\":'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_list[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
