{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d042e3da-baa2-4781-b7fe-0fc4aa0548d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_methods import *\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0bbd6a57-4cbf-458a-8fa5-1d6d6178714c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126834/2745535329.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_entropies = torch.load(\"runs/all_entropies.pt\")\n"
     ]
    }
   ],
   "source": [
    "all_entropies = torch.load(\"runs/all_entropies.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "29b7f4d0-543f-4e36-b7d4-3b9d99b3bf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([215, 32000])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entropies.shape # Note that the new shape reflects we minimised over the layers dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "df9c5325-959a-433a-8e73-5f40d9e058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,_,context_outputs,memory_outputs = load_memotrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b53cc2-37ca-4d95-b12a-73702c37b8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b0501e14b441e9cb5137b70adabdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,torch_dtype=torch.float16).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "activations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf855b3-87a5-4fa8-b926-9feea093e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "prompt = inputs[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4edcbda1-4d69-4f57-9c6e-a7f37b88b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "tokenizer.pad_token = \"<s>\"\n",
    "eos_token = tokenizer.eos_token_id\n",
    "input_ids = tokenizer(prompt,return_tensors=\"pt\",padding=True).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dda79c2-1205-4425-b59a-33012a533c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "last_token_logits = model(input_ids).logits[0,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011312ae-8915-41c0-a7a7-431dff787a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126834/2879142810.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n"
     ]
    }
   ],
   "source": [
    "last_token_probs = F.softmax(last_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf1fae0-70dd-4969-9b1e-0596186526af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a quote that ends in the word \"fur\": A rag and a bone and a hank of'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d54fc876-a6a9-41a5-bd98-d381563b47bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [1, 2982], 'attention_mask': [1, 1]},\n",
       " {'input_ids': [1, 3691], 'attention_mask': [1, 1]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"fur\"\n",
    "memory = \"hair\"\n",
    "tokenizer(\"fur\"),tokenizer(\"hair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2810bac-f969-40f5-9202-6b825bf3c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob('fur') = 0.004942772909998894\n",
      "Prob('hair') = 0.9871297478675842\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prob('fur') = {last_token_probs[2982].item()}\\nProb('hair') = {last_token_probs[3691].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c3bea-b0bc-42a2-8698-1bb5824abd07",
   "metadata": {},
   "source": [
    "I'm going to do entropy-based penalisation to ensure \"fur\" is of higher probability and hair is of lower probability. I need to choose alpha that ensures this. Note that the assumption is that fur's entropy is minimum across all tokens. I don't think this assumption is valid, but if we initially ignore all tokens with very low probabilities (the minimum prob tokens), then the entropy thing may work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16005ab9-9eca-4be1-8ee1-47eb29cb9caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4849066497880004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "l = 12 # length of the fur/hair context.\n",
    "math.log(l) # this tells what the maximum possible entropy is, if the entire row is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4413d2e-dbf8-4843-ae91-47f413f25161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4849, 2.4849, 2.4849,  ..., 2.4849, 2.4849, 2.4849])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entropies[id] # Note how most vocab tokens have the max possible entropy. Only a few context tokens will differ from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d1aa5ec-2c72-4773-a478-8b19c517302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = torch.nonzero(all_entropies[id] < 2.4849).squeeze().tolist()\n",
    "words_strings = tokenizer.batch_decode(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f934ec30-2f22-4661-beae-835710134255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, d, in, \", that, with, an, ', from, up, about, ious, “, every, down, word, program, report, following, short, enc, sum, code, example, story, fur, write, words, hope, write, ‘, ha, anger, song, review, express, written, capt, wrote, Write, excl, letter, insp, blog, statement, describe, script, brief, ended, reflect, iously, …, Fur, motiv, letters, ends, atr, essay, inspired, Write, sentence, ending, quote, writes, describes, ell, smith, phrase, persu, courage, poem, reson, wisdom, quot, summar, paragraph, dialogue, fitting, quotes, punct, â, essays, sentences, ugno, accurately, reflects, rhet, dash, attributed, inspire, fur, …, \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for word in words_strings:\n",
    "    print(word,end=\", \")\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e46638",
   "metadata": {},
   "source": [
    "- One thing to note is that very few words have an entropy that even deviates slightly. That's a good sign.\n",
    "- Furthermore, a lot of these words seem closely related to the words in the context. For example, the word \"write\" as well as the word \"written\", \"review\", \"express\" are all similar. I guess this is what we need for our multi-token thing too.\n",
    "- There are also a lot of stop words.\n",
    "\n",
    "Next step, see if there are any very-low-probability tokens here. I guess we want to eliminate those before modifying them.\n",
    "\n",
    "Because what the paper also says is to perform a filtering operation first (like in DoLa), by reducing those lowest probability tokens to zero, then softmaxing to normalise the remaining tokens, and finally performing the entropy penalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e85616e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6104830137919635e-05 a\n",
      "1.289014471694827e-05 in\n",
      "1.8788913394018891e-06 \"\n",
      "3.7512456856347853e-06 that\n",
      "4.8166953092732e-06 '\n",
      "3.0616777166869724e-06 every\n",
      "0.004942772909998894 fur\n",
      "2.755211653493461e-06 hope\n",
      "1.1575513099160162e-06 ‘\n",
      "6.583642061741557e-06 ha\n",
      "6.532407496706583e-06 Fur\n",
      "2.776821247607586e-06 courage\n",
      "1.0770118933578487e-05 fur\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(words_strings)):\n",
    "    if last_token_probs[words][i].item() > 9.87e-7:\n",
    "        print(last_token_probs[words][i].item(), words_strings[i])\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f1177",
   "metadata": {},
   "source": [
    "This filtering mechanism is really good, because it shows that words like \"write\" which scored high on entropy (due to presence in the context) scored low on output probability (not a very viable next token). Such tokens can be eliminated immediately.\n",
    "\n",
    "Since the max prob is 0.987, we need an alpha such that threshold becomes 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973e328a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.87e-07"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 1e-6 # Let it be 1e-6, just an approximation.\n",
    "alpha*0.987"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecda1a2",
   "metadata": {},
   "source": [
    "1. Filter out according to this $\\alpha=10^{-6}$ by setting the lower ones to -inf.\n",
    "2. Softmax all tokens so these ones are now zero.\n",
    "3. Apply the exponential decay on probabilities with some hyperparameter $\\beta$. Only the non-zero ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ae37432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46261d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9871, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(last_token_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a29e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126834/3222005376.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  filtered_probs = F.softmax(filtered_logits)\n"
     ]
    }
   ],
   "source": [
    "def filter_low_prob_tokens(input_logits):\n",
    "    indices_to_filter = torch.nonzero(last_token_probs<alpha*torch.max(last_token_probs)).squeeze()\n",
    "    filtered_logits = input_logits[:]\n",
    "    filtered_logits[indices_to_filter] = float('-inf')\n",
    "    filtered_probs = F.softmax(filtered_logits)\n",
    "    return filtered_probs\n",
    "filtered_probs = filter_low_prob_tokens(last_token_logits).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c0bacd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_context_length(id):\n",
    "    input_ids = tokenizer(inputs[id]).input_ids\n",
    "    input_tokens = tokenizer.batch_decode(input_ids)\n",
    "    # print(input_tokens)\n",
    "    # print(input_tokens.index(\"\\\":\"))\n",
    "    return input_tokens.index(\"\\\":\")+1\n",
    "get_context_length(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa1624c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with smallest entropy:  2982\n",
      "Token with max probab:  3691\n",
      "\n",
      "Maximum probability token (subtraction amt per beta):   -2.384185791015625e-07\n",
      "Minimum entropy token (subtraction amt per beta):  -0.07077932357788086\n",
      "\n",
      "Maximum probability token (prior probab):  3691 0.9877297282218933\n",
      "Minimum entropy token (prior probab):  2982 0.004945777356624603\n",
      "\n",
      "Maximum probability token (posterior prob):  3691 0.25430625677108765\n",
      "Minimum entropy token (posterior prob):  2982 0.7438071966171265\n"
     ]
    }
   ],
   "source": [
    "def scale_prob_by_entropy(beta,probs):\n",
    "    # probs is a vector 32000-dim\n",
    "    entropies = all_entropies[id].to(\"cuda\")\n",
    "    # print(entropies)\n",
    "    subtract = entropies-math.log(get_context_length(id))\n",
    "    min_entropy_token = torch.argmin(subtract).item()\n",
    "    max_prior_prob_token = torch.argmax(probs).item()\n",
    "\n",
    "    print(\"Token with smallest entropy: \",min_entropy_token) # If this is 2982, it shows that \"fur\" is the most likely predicted according to entropy among the tokens that have high enough output probability\n",
    "    print(\"Token with max probab: \",max_prior_prob_token)\n",
    "    print(\"\\nMaximum probability token (subtraction amt per beta):  \",subtract[max_prior_prob_token].item())\n",
    "    print(\"Minimum entropy token (subtraction amt per beta): \",subtract[min_entropy_token].item())\n",
    "    print()\n",
    "    \n",
    "    print(\"Maximum probability token (prior probab): \",max_prior_prob_token,probs[max_prior_prob_token].item())\n",
    "    print(\"Minimum entropy token (prior probab): \",min_entropy_token,probs[min_entropy_token].item())\n",
    "    print()\n",
    "    logs = torch.log(probs)\n",
    "    # print(\"Maximum probability token (prior logs): \",max_prior_prob_token,logs[max_prior_prob_token].item())\n",
    "    # print(\"Minimum entropy token (prior logs): \",min_entropy_token,logs[min_entropy_token].item())\n",
    "    # print()\n",
    "    logs -= beta*subtract\n",
    "    # print(\"Maximum probability token (posterior logs): \",max_prior_prob_token,logs[max_prior_prob_token].item())\n",
    "    # print(\"Minimum entropy token (posterior logs): \",min_entropy_token,logs[min_entropy_token].item())\n",
    "    # print()\n",
    "    final_probs = F.softmax(logs,dim=0)\n",
    "    print(\"Maximum probability token (posterior prob): \",max_prior_prob_token,final_probs[max_prior_prob_token].item())\n",
    "    print(\"Minimum entropy token (posterior prob): \",min_entropy_token,final_probs[min_entropy_token].item())\n",
    "    return final_probs\n",
    "final_probs = scale_prob_by_entropy(90,filtered_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43661cf9",
   "metadata": {},
   "source": [
    "We see the 3691'th token got scaled down and 2982th token got scaled up. If we increase beta, we can scale it such that 2982th is above 3691th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c854f92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fur']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greedy decoding\n",
    "tokenizer.batch_decode([torch.argmax(final_probs).tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4628d2",
   "metadata": {},
   "source": [
    "Great! Now we try it on another input id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "469b24c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126834/3960575449.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write a quote that ends in the word \"boat\": Rats desert a sinking'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 2\n",
    "\n",
    "prompt = inputs[id]\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer.pad_token = \"<s>\"\n",
    "eos_token = tokenizer.eos_token_id\n",
    "input_ids = tokenizer(prompt,return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "last_token_logits = model(input_ids).logits[0,-1,:]\n",
    "last_token_probs = F.softmax(last_token_logits)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca51a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ", a, d, in, \", that, with, an, ', from, up, about, “, every, down, word, load, program, report, following, short, enc, sum, code, story, write, words, hope, write, ‘, ha, anger, song, review, express, written, capt, wrote, Write, excl, letter, insp, blog, statement, describe, script, brief, ended, reflect, …, motiv, boat, letters, ends, atr, essay, inspired, Write, sentence, ending, quote, writes, describes, ell, quote, smith, phrase, persu, courage, poem, reson, wisdom, quot, summar, paragraph, dialogue, boats, fitting, dock, quotes, punct, â, essays, sentences, accurately, reflects, rhet, dash, attributed, inspire, loads, …, \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "words = torch.nonzero(all_entropies[id] < 2.4849).squeeze().tolist()\n",
    "words_strings = tokenizer.batch_decode(words)\n",
    "for word in words_strings:\n",
    "    print(word,end=\", \")\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e179fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "932f6581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126834/3222005376.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  filtered_probs = F.softmax(filtered_logits)\n"
     ]
    }
   ],
   "source": [
    "def filter_low_prob_tokens(input_logits):\n",
    "    indices_to_filter = torch.nonzero(last_token_probs<alpha*torch.max(last_token_probs)).squeeze()\n",
    "    filtered_logits = input_logits[:]\n",
    "    filtered_logits[indices_to_filter] = float('-inf')\n",
    "    filtered_probs = F.softmax(filtered_logits)\n",
    "    return filtered_probs\n",
    "filtered_probs = filter_low_prob_tokens(last_token_logits).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "45eaf5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.53e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiplications_needed(a):\n",
    "    print(a)\n",
    "    return math.ceil(-1 - math.log10(a))\n",
    "\n",
    "multiplications_needed(0.00000253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8fe513b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_prob_by_entropy(id,beta,probs):\n",
    "    wordC,wordM = context_outputs[id],memory_outputs[id]\n",
    "    wordC_token = tokenizer(wordC).input_ids[1]\n",
    "    wordM_token = tokenizer(wordM).input_ids[1]\n",
    "    print(f\"Prob (context token) = {probs[wordC_token]}\")\n",
    "    print(f\"Context: {wordC},{wordC_token}\\nMemory: {wordM},{wordM_token}\\n\")\n",
    "\n",
    "    entropies = all_entropies[id].to(\"cuda\")\n",
    "    subtract = entropies-math.log(get_context_length(id))\n",
    "\n",
    "    # Min entropy among those tokens whose probabilities are non-zero (exclude tokens which are filtered out)\n",
    "    min_entropy_token = torch.argmin(torch.where(probs > 0.0, subtract, torch.tensor(float('inf')))).item()\n",
    "    max_prior_prob_token = torch.argmax(probs).item()\n",
    "\n",
    "    multiply = math.ceil(-1 - math.log10(-subtract[min_entropy_token].item()))\n",
    "    subtract *= 10**(multiply) # Multiply full thing by a constant\n",
    "\n",
    "    print(\"Token with smallest entropy: \",min_entropy_token) # If this is 2982, it shows that \"fur\" is the most likely predicted according to entropy among the tokens that have high enough output probability\n",
    "    print(\"Token with max probab: \",max_prior_prob_token)\n",
    "    print(\"\\nMaximum probability token (subtraction amt per beta):  \",subtract[max_prior_prob_token].item())\n",
    "    print(\"Minimum entropy token (subtraction amt per beta): \",subtract[min_entropy_token].item())\n",
    "    print()\n",
    "    \n",
    "    print(\"Maximum probability token (prior probab): \",max_prior_prob_token,probs[max_prior_prob_token].item())\n",
    "    print(\"Minimum entropy token (prior probab): \",min_entropy_token,probs[min_entropy_token].item())\n",
    "    print()\n",
    "    logs = torch.log(probs)\n",
    "    logs -= beta*subtract\n",
    "    final_probs = F.softmax(logs,dim=0)\n",
    "    print(\"Maximum probability token (posterior prob): \",max_prior_prob_token,final_probs[max_prior_prob_token].item())\n",
    "    print(\"Minimum entropy token (posterior prob): \",min_entropy_token,final_probs[min_entropy_token].item())\n",
    "    return final_probs\n",
    "# final_probs = scale_prob_by_entropy(40,filtered_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a2d08",
   "metadata": {},
   "source": [
    "Now trying another input_id ('child', which was a bit adversarial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5908710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_prob_tokens(input_logits,alpha):\n",
    "    indices_to_filter = torch.nonzero(last_token_probs<alpha*torch.max(last_token_probs)).squeeze()\n",
    "    filtered_logits = input_logits[:]\n",
    "    filtered_logits[indices_to_filter] = float('-inf')\n",
    "    filtered_probs = F.softmax(filtered_logits)\n",
    "    return filtered_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c9bdcb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob (context token) = 1.0947568625852e-05\n",
      "Context: child,1502\n",
      "Memory: bull,10386\n",
      "\n",
      "Token with smallest entropy:  13\n",
      "Token with max probab:  10386\n",
      "\n",
      "Maximum probability token (subtraction amt per beta):   0.0\n",
      "Minimum entropy token (subtraction amt per beta):  -0.2956390380859375\n",
      "\n",
      "Maximum probability token (prior probab):  10386 0.9994819760322571\n",
      "Minimum entropy token (prior probab):  13 1.7254038766623125e-06\n",
      "\n",
      "Maximum probability token (posterior prob):  10386 0.8087160587310791\n",
      "Minimum entropy token (posterior prob):  13 0.1908482313156128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126834/374731686.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n",
      "/tmp/ipykernel_126834/4064296362.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  filtered_probs = F.softmax(filtered_logits)\n"
     ]
    }
   ],
   "source": [
    "id = 1\n",
    "\n",
    "prompt = inputs[id]\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer.pad_token = \"<s>\"\n",
    "eos_token = tokenizer.eos_token_id\n",
    "input_ids = tokenizer(prompt,return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "last_token_logits = model(input_ids).logits[0,-1,:]\n",
    "last_token_probs = F.softmax(last_token_logits)\n",
    "alpha = 1e-6\n",
    "\n",
    "filtered_probs = filter_low_prob_tokens(last_token_logits,alpha).detach()\n",
    "final_probs = scale_prob_by_entropy(id,40,filtered_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a8933a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bc5d95c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3234"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(3.3234123,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3f2063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4849066497880004"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "math.log(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7645794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000e-07, -1.0000e-07, -4.4900e-02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps=1e-7\n",
    "thresh = 0.005\n",
    "output_entropies = torch.tensor([2.4400,2.4780,2.4849])\n",
    "entropies = torch.tensor([[2.4400,2.4780,2.44]])\n",
    "torch.where(math.log(12) - output_entropies > thresh,-eps,entropies-output_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d25df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
