{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c88d638-5bb8-4501-a703-c0ab3545613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import math\n",
    "from classes.common_methods import *\n",
    "import copy\n",
    "import gc\n",
    "from classes.RefusalSteering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a3fe7-909e-4fbb-a82e-3966c7e93c91",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae68dc46-e979-45ed-b325-7356c9e820cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baf92500981472bb8ae766f153cf9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen-1_8B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "act_add_model = copy.deepcopy(model)\n",
    "dir_abl_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccd98343-cb5a-49af-af04-fa6e4e71bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = torch.load(\"refusal_direction_paper/direction_qwen.pt\")\n",
    "dir.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b4727-a0a0-452a-bb39-fac5deceb4e8",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656499ed-5f1a-477c-8551-5451e8672231",
   "metadata": {},
   "source": [
    "### Confirming that direction matches with the overall means_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfa28b80-cc00-401e-89b6-03c12650f528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = torch.load(\"refusal_direction_paper/direction_qwen.pt\")\n",
    "dir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13e7ec7d-9fa6-4c2c-aa65-9e697b46cbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 24, 2048])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diffs = torch.load(\"refusal_direction_paper/mean_diffs_qwen.pt\")\n",
    "mean_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6030863f-ccf1-4ce6-a0b5-bf76d2a9f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_diffs2 = torch.load(\"refusal_direction_paper/mean_diffs_llama_7b.pt\")\n",
    "# mean_diffs2.shape\n",
    "# torch.Size([6,32,4096])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26323c8b-5d91-4daf-b784-c350f65635f2",
   "metadata": {},
   "source": [
    "I think they took 24 layers and just 5 tokens.\n",
    "5 tokens would've taken way less time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b911ffd1-523e-445a-8337-672745989b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3309, -0.8965,  1.2525,  ..., -0.5334,  0.2105, -0.0733],\n",
       "        device='cuda:0', dtype=torch.float64),\n",
       " tensor([ 0.3309, -0.8965,  1.2525,  ..., -0.5334,  0.2105, -0.0733],\n",
       "        device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diffs[-2,15,:],dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddee4f-0b8b-4e83-82d0-74b17e61edda",
   "metadata": {},
   "source": [
    "In the paper they said 15/24 and -1 token. -1 is the second last token (corresponds to id -2).\n",
    "The 15/24 is actually 16th layer out of 24 with id = 15 (incorrectly mentioned).\n",
    "\n",
    "It matches, but make sure to remember this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcbb21-0fa7-463b-b632-40911c411a00",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757b468d-c4d4-43f5-b773-12b8237e33ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (w2): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (c_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8f2d31-6ed6-4ce2-89ec-362c38cf6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_activation_addn_model(l_opt,r_opt):\n",
    "    for l in range(len(act_add_model.transformer.h)):\n",
    "        if l==l_opt:\n",
    "            act_add_model.transformer.h[l] = SteeringLayer(model.config.hidden_size,model.transformer.h[l],r_opt,type=\"addition\")\n",
    "        else:\n",
    "            act_add_model.transformer.h[l] = model.transformer.h[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adf42b3-e650-4719-a994-86e405563d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_directional_ablation_model(r_opt):\n",
    "    for l in range(len(dir_abl_model.transformer.h)):\n",
    "        dir_abl_model.transformer.h[l] = SteeringLayer(model.config.hidden_size,model.transformer.h[l],r_opt,type=\"ablation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbcca59a-b075-4fce-addc-650bce1e5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_decoding(prompt,debug=True, max_tokens=1,\n",
    "                     show_tqdm=True, return_prob=False,type=\"addition\"):\n",
    "    model_to_use = act_add_model if type==\"addition\" else dir_abl_model\n",
    "    print(\"Using model...\")\n",
    "    # print(model_to_use)\n",
    "    if debug: print(\"prompt: \",prompt)\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    eos_token = tokenizer.eos_token_id\n",
    "    # tokenizer.pad_token = \"<s>\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt,return_tensors=\"pt\").input_ids.to(device)\n",
    "    predicted_tokens = []\n",
    "\n",
    "    token_iterator = tqdm(range(max_tokens)) if (max_tokens>1 and show_tqdm) else range(max_tokens)\n",
    "    \n",
    "    for token in token_iterator:\n",
    "        last_token_logits = model_to_use(input_ids).logits[0,-1,:]\n",
    "        last_token_probs = F.softmax(last_token_logits)\n",
    "\n",
    "        max_index = torch.argmax(last_token_probs).item() # greedy decoding\n",
    "        \n",
    "        if max_index == eos_token:\n",
    "            break\n",
    "\n",
    "        predicted_tokens.append(max_index)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids,torch.tensor([[max_index]]).to(device)],dim=1)\n",
    "    \n",
    "    out = tokenizer.decode(predicted_tokens) if max_tokens>1 \\\n",
    "                else tokenizer.batch_decode(predicted_tokens)[0]\n",
    "    if debug: print(\"output: \",out)\n",
    "    if not debug:\n",
    "        if return_prob==True and max_tokens==1:\n",
    "            return last_token_probs\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c5ab874-f0c3-4fec-94a7-46a0b79bf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ds_refusal/splits/harmful_val.json', 'r') as file:\n",
    "    dval_harmful = json.load(file)\n",
    "\n",
    "with open('ds_refusal/splits/harmless_val.json', 'r') as file:\n",
    "    dval_harmless = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0be59bf8-4683-4476-a3aa-ad47860a338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60709/3629357990.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  last_token_probs = F.softmax(last_token_logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<inst>Describe the characteristics of the star and flag rating system.</inst><assistant>The characteristics of the star and flag rating system are\n"
     ]
    }
   ],
   "source": [
    "refused = 0\n",
    "i = 4\n",
    "pt = dval_harmless[i]\n",
    "l_opt = 15 # layer id 15 is the 16th layer. But that's the one.\n",
    "r_opt = dir\n",
    "set_activation_addn_model(l_opt,r_opt)\n",
    "prompt='<inst>'+pt['instruction']+'</inst><assistant>'\n",
    "\n",
    "toks=10\n",
    "out = regular_decoding(prompt,debug=False, max_tokens=toks,\n",
    "                     show_tqdm=False, return_prob=False,type=\"addition\")\n",
    "\n",
    "print(prompt,end=\"\")\n",
    "print(out)\n",
    "refused+=1\n",
    "    \n",
    "# print(\"***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05624225-f802-43aa-9319-947fc383d493",
   "metadata": {},
   "source": [
    "**Conclusion:** I used the exact direction they gave and it doesn't seem to be working even now. There must be some issue in my Steering implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526a1be-3c2c-4647-9a4f-97916993d9b3",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
